# Hadoop Project

This project utilizes the following technologies:
- Apache Spark
- Apache Hive
- HDFS (Hadoop Distributed File System)
- Flask

## Overview

This project aims to demonstrate the integration of big data technologies with a web framework to process and visualize large datasets.

## Technologies

### Apache Spark
Spark is used for large-scale data processing and analytics.

### Apache Hive
Hive is used for data warehousing and SQL-like querying of large datasets stored in HDFS.

### HDFS
HDFS is the storage layer of the Hadoop ecosystem, used to store large datasets across a distributed cluster.

### Flask
Flask is a lightweight web framework used to create a web interface for data visualization and interaction.

## Setup

1. Install Apache Spark, Hive, and Hadoop.
2. Set up a virtual environment and install Flask.
3. Configure the necessary environment variables and paths.

## Usage

1. Start the Hadoop and Hive services.
2. Run the Spark jobs to process the data.
3. Launch the Flask application to visualize the results.

## Contributing

Feel free to submit issues or pull requests if you have any improvements or bug fixes.

## License

This project is licensed under the MIT License.